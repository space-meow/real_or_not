{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Activation, LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization Regexes\n",
    "number_re = re.compile(r\"(?:\\d+[,\\.\\d]*)?\\d\")\n",
    "punct_re = re.compile(r\"[!@#\\$%\\^&\\*\\(\\)\\-_\\+=\\{\\}\\[\\]:;\\\"',<\\.>\\\\/\\?]\")\n",
    "multi_space_re = re.compile(r\"\\s\\s+\")\n",
    "hashtag_re = re.compile(r\"#[^\\s]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(path):\n",
    "    '''\n",
    "    Loads the word embedding into memory\n",
    "    '''\n",
    "    embeddings_index = {}\n",
    "    with open(path, \"r\") as _f:\n",
    "        for line in _f:\n",
    "            values = line.rstrip().rsplit(\" \")\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    print(\"{} word vectors\".format(len(embeddings_index)))\n",
    "    return embeddings_index\n",
    "\n",
    "def set_up_weight_matrix(embedding_index, word_index):\n",
    "    '''\n",
    "    Sets up the weight matrix for the embedding layer by creating a new matrix and filling it\n",
    "    with the vectors from the word embedding.\n",
    "    '''\n",
    "    words_not_found = set()\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.zeros((nb_words, 300))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i == 0:\n",
    "            print(word)\n",
    "        if i >= nb_words:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = embedding_index[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            words_not_found.add(word)\n",
    "    print(\"{} words not found out of {} words\".format(len(words_not_found), len(word_index)))\n",
    "    return embedding_matrix\n",
    "\n",
    "def tokenize_texts(texts):\n",
    "    tokenized = []\n",
    "    for text in texts:\n",
    "        text = multi_space_re.sub(\" \", number_re.sub(\"\", punct_re.sub(\"\", hashtag_re.sub(\"\", text.lower())))).strip()\n",
    "        tokenized.append(text)\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2519371 word vectors\n"
     ]
    }
   ],
   "source": [
    "en_vec = load_embedding(\"../../Utils/word_vectors/wiki.en.align.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"tokenized\"] = tokenize_texts(train_df[\"text\"])\n",
    "test_df[\"tokenized\"] = tokenize_texts(test_df[\"text\"])\n",
    "all_texts = train_df[\"tokenized\"].tolist() + test_df[\"tokenized\"].tolist()\n",
    "train_texts = train_df[\"tokenized\"].tolist()\n",
    "train_labs = train_df[\"target\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(lower=True, char_level=False)\n",
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = .9\n",
    "split_point = int(len(train_texts) * val_size)\n",
    "x_texts = train_texts[:split_point]\n",
    "y_labs = train_labs[:split_point]\n",
    "val_texts = train_texts[split_point:]\n",
    "val_labs = train_labs[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_texts_tok = tokenizer.texts_to_sequences(x_texts)\n",
    "x_texts_tok = sequence.pad_sequences(x_texts_tok, maxlen=20)\n",
    "val_texts_tok = tokenizer.texts_to_sequences(val_texts)\n",
    "val_texts_tok = sequence.pad_sequences(val_texts_tok, maxlen=20)\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_texts_tok = np.array(x_texts_tok)\n",
    "y_labs = np.array(y_labs)\n",
    "val_texts_tok = np.array(val_texts_tok)\n",
    "val_labs = np.array(val_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11620 words not found out of 26619 words\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = set_up_weight_matrix(en_vec, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = Sequential()\n",
    "lstm.add(Embedding(len(word_index)+1,\n",
    "                   300,\n",
    "                   weights=[embedding_matrix],\n",
    "                   input_length=20,\n",
    "                   trainable=False))\n",
    "lstm.add(Dense(512, activation=\"relu\"))\n",
    "lstm.add(Dropout(0.2))\n",
    "#lstm.add(LSTM(256))\n",
    "#lstm.add(LSTM(256, dropout = 0.3, recurrent_dropout = 0.3))\n",
    "lstm.add(LSTM(256, dropout = 0.2))\n",
    "lstm.add(Dense(256, activation=\"relu\"))\n",
    "lstm.add(Dropout(0.3))\n",
    "lstm.add(Dense(1, activation='sigmoid'))\n",
    "lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "215/215 [==============================] - 28s 128ms/step - loss: 0.5108 - acc: 0.7597 - val_loss: 0.4750 - val_acc: 0.7927\n",
      "Epoch 2/20\n",
      "215/215 [==============================] - 26s 122ms/step - loss: 0.4280 - acc: 0.8210 - val_loss: 0.4225 - val_acc: 0.8045\n",
      "Epoch 3/20\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.4121 - acc: 0.8251 - val_loss: 0.4342 - val_acc: 0.7979\n",
      "Epoch 4/20\n",
      "215/215 [==============================] - 27s 126ms/step - loss: 0.3850 - acc: 0.8351 - val_loss: 0.4351 - val_acc: 0.7953\n",
      "Epoch 5/20\n",
      "215/215 [==============================] - 31s 143ms/step - loss: 0.3500 - acc: 0.8567 - val_loss: 0.4361 - val_acc: 0.7940\n",
      "Epoch 6/20\n",
      "215/215 [==============================] - 33s 152ms/step - loss: 0.3276 - acc: 0.8651 - val_loss: 0.4839 - val_acc: 0.7782\n",
      "Epoch 7/20\n",
      "215/215 [==============================] - 32s 150ms/step - loss: 0.2962 - acc: 0.8807 - val_loss: 0.5189 - val_acc: 0.7874\n",
      "Epoch 8/20\n",
      "215/215 [==============================] - 32s 147ms/step - loss: 0.2714 - acc: 0.8943 - val_loss: 0.5135 - val_acc: 0.7743\n",
      "Epoch 9/20\n",
      "215/215 [==============================] - 32s 148ms/step - loss: 0.2464 - acc: 0.8996 - val_loss: 0.7241 - val_acc: 0.7651\n",
      "Epoch 10/20\n",
      "215/215 [==============================] - 30s 138ms/step - loss: 0.2299 - acc: 0.9101 - val_loss: 0.5639 - val_acc: 0.7717\n",
      "Epoch 11/20\n",
      "215/215 [==============================] - 32s 150ms/step - loss: 0.2028 - acc: 0.9200 - val_loss: 0.6305 - val_acc: 0.7677\n",
      "Epoch 12/20\n",
      "215/215 [==============================] - 32s 149ms/step - loss: 0.1813 - acc: 0.9296 - val_loss: 0.7087 - val_acc: 0.7572\n",
      "Epoch 13/20\n",
      "215/215 [==============================] - 33s 152ms/step - loss: 0.1765 - acc: 0.9270 - val_loss: 0.6904 - val_acc: 0.7887\n",
      "Epoch 14/20\n",
      "215/215 [==============================] - 28s 132ms/step - loss: 0.1580 - acc: 0.9336 - val_loss: 0.7183 - val_acc: 0.7848\n",
      "Epoch 15/20\n",
      "215/215 [==============================] - 28s 130ms/step - loss: 0.1463 - acc: 0.9404 - val_loss: 0.8167 - val_acc: 0.7651\n",
      "Epoch 16/20\n",
      "215/215 [==============================] - 31s 145ms/step - loss: 0.1401 - acc: 0.9437 - val_loss: 0.7585 - val_acc: 0.7808\n",
      "Epoch 17/20\n",
      "215/215 [==============================] - 32s 148ms/step - loss: 0.1298 - acc: 0.9460 - val_loss: 0.8550 - val_acc: 0.7848\n",
      "Epoch 18/20\n",
      "215/215 [==============================] - 31s 145ms/step - loss: 0.1287 - acc: 0.9485 - val_loss: 0.8140 - val_acc: 0.7927\n",
      "Epoch 19/20\n",
      "215/215 [==============================] - 32s 150ms/step - loss: 0.1141 - acc: 0.9548 - val_loss: 0.8430 - val_acc: 0.7874\n",
      "Epoch 20/20\n",
      "215/215 [==============================] - 32s 150ms/step - loss: 0.1071 - acc: 0.9539 - val_loss: 0.9879 - val_acc: 0.7782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5e719f2e50>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fit(x_texts_tok, y_labs, epochs=20, validation_data=(val_texts_tok, val_labs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 1s 51ms/step - loss: 0.9879 - acc: 0.7782\n",
      "77.8215229511261\n"
     ]
    }
   ],
   "source": [
    "scores = lstm.evaluate(val_texts_tok, val_labs)\n",
    "print(scores[1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
